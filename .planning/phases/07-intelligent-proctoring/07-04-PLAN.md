---
phase: 07-intelligent-proctoring
plan: 04
type: execute
wave: 3
depends_on: ["07-03"]
files_modified:
  - lib/proctoring/snapshotAnalysis.ts
  - workers/proctoring-worker.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "BullMQ worker processes snapshot analysis jobs from the proctoring-analysis queue"
    - "GPT-4 Vision analyzes snapshots for face count, looking away, and suspicious behavior"
    - "AI flags create ProctorEvent records with confidence scores and snapshot URLs"
    - "Worker respects sensitivity setting (low/medium/high)"
    - "Worker handles errors gracefully with retries"
    - "Parse errors return a safe default SnapshotAnalysisResult with confidence 0"
  artifacts:
    - path: "lib/proctoring/snapshotAnalysis.ts"
      provides: "GPT-4 Vision snapshot analysis function"
      min_lines: 40
    - path: "workers/proctoring-worker.ts"
      provides: "BullMQ worker for async proctoring analysis"
      min_lines: 60
  key_links:
    - from: "workers/proctoring-worker.ts"
      to: "lib/proctoring/snapshotAnalysis.ts"
      via: "import analyzeSnapshot"
      pattern: "analyzeSnapshot"
    - from: "workers/proctoring-worker.ts"
      to: "prisma.proctorEvent"
      via: "create ProctorEvent for each flag"
      pattern: "proctorEvent.create"
    - from: "lib/proctoring/snapshotAnalysis.ts"
      to: "openai"
      via: "chat.completions.create with image_url"
      pattern: "chat.completions.create"
---

<objective>
Build the async AI snapshot analysis pipeline: a GPT-4 Vision analysis function and a BullMQ worker that processes snapshot jobs, detects suspicious behavior, and creates ProctorEvent records with confidence scores.

Purpose: PROCT-02 AI analysis. Analyzes webcam snapshots asynchronously to detect looking away, multiple faces, and absence without impacting exam performance.
Output: Snapshot analysis function and BullMQ worker that creates flagged ProctorEvents.
</objective>

<execution_context>
@C:\Users\hugol\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\hugol\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/07-intelligent-proctoring/07-RESEARCH.md
@.planning/phases/07-intelligent-proctoring/07-01-SUMMARY.md
@.planning/phases/07-intelligent-proctoring/07-03-SUMMARY.md
@lib/queue.ts
@lib/proctoring/types.ts
@lib/antiCheat.ts
@app/api/attempts/[id]/proctor-events/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPT-4 Vision snapshot analysis function</name>
  <files>
    lib/proctoring/snapshotAnalysis.ts
  </files>
  <action>
1. Create `lib/proctoring/snapshotAnalysis.ts`:
   - Import OpenAI from 'openai' (already in project dependencies — used in Phase 4 grading)
   - Import SnapshotAnalysisResult, ProctoringConfig from './types'
   - Find the existing OpenAI client pattern in the codebase (grep for "new OpenAI" to see how it's initialized) and follow that pattern

2. Export `async function analyzeSnapshot(snapshotUrl: string, sensitivity: ProctoringConfig['sensitivity']): Promise<SnapshotAnalysisResult>`:
   - Use GPT-4 Vision (model: "gpt-4o" which supports vision — NOT "gpt-4-vision-preview" which is deprecated)
   - System prompt: "You are an exam proctoring assistant. Analyze this webcam snapshot of a student taking an exam. Be objective and factual. Do not identify the person."
   - User prompt with image:
     ```
     Analyze this exam proctoring snapshot. Report:
     1. Number of faces visible (0 = student absent, 1 = normal, 2+ = multiple people)
     2. Is the student looking away from the screen? (eyes directed elsewhere for extended period)
     3. Any other suspicious behavior? (phone visible, notes visible, another screen)

     Respond ONLY with valid JSON:
     {"faces": <number>, "looking_away": <boolean>, "suspicious": <boolean>, "description": "<brief factual description>"}
     ```
   - Pass the image via image_url content part with the snapshotUrl
   - max_tokens: 200 (keep response short)
   - temperature: 0 (deterministic analysis)
   - Parse the JSON response. On parse error (invalid JSON, missing fields, or any exception during parsing), return a safe default:
     ```typescript
     const DEFAULT_ANALYSIS: SnapshotAnalysisResult = {
       faces: 0,
       lookingAway: false,
       suspicious: false,
       description: 'Analysis failed: unable to parse AI response',
       confidence: 0
     }
     ```
     Do NOT throw on parse errors — return the default with confidence 0 so that shouldFlag produces no flags (since confidence 0 signals unreliable data).
   - For successful parses, calculate confidence: 0.85 for clear images (default), reduce if response is ambiguous
   - Return SnapshotAnalysisResult

3. Export `function shouldFlag(result: SnapshotAnalysisResult, sensitivity: ProctoringConfig['sensitivity']): string[]`:
   - Returns array of ProctorEventType strings to create
   - Sensitivity 'low': only flag ABSENCE (faces === 0) and MULTIPLE_FACES (faces > 1)
   - Sensitivity 'medium': also flag LOOKING_AWAY (when looking_away === true)
   - Sensitivity 'high': also flag SUSPICIOUS_BEHAVIOR (when suspicious === true)
   - Always include the description in event metadata regardless of sensitivity
   - If confidence === 0, return empty array (unreliable analysis should produce no flags)

IMPORTANT: Use the same OpenAI SDK version and initialization pattern as the existing grading code. The project uses `openai` npm package with `chat.completions.create` (NOT the legacy API).
  </action>
  <verify>
    Run `npx tsc --noEmit` — no type errors.
    Grep for "gpt-4o" in snapshotAnalysis.ts.
    Grep for "shouldFlag" in snapshotAnalysis.ts.
    Grep for "confidence: 0" in snapshotAnalysis.ts to confirm safe default on parse error.
  </verify>
  <done>
    analyzeSnapshot calls GPT-4 Vision and returns structured analysis. On parse error, returns default SnapshotAnalysisResult with confidence 0 (no throw). shouldFlag filters results by sensitivity level and skips flagging when confidence is 0.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create BullMQ proctoring worker</name>
  <files>
    workers/proctoring-worker.ts
    package.json
  </files>
  <action>
1. Find the existing BullMQ worker in the project (grep for "Worker" and "bullmq" in non-node_modules .ts files). Study the pattern for how workers are structured and started.

2. Create `workers/proctoring-worker.ts`:
   - Import Worker, Job from 'bullmq'
   - Import Redis from 'ioredis'
   - Import prisma from '@/lib/prisma' (or however the existing worker imports it)
   - Import analyzeSnapshot, shouldFlag from '@/lib/proctoring/snapshotAnalysis'
   - Import ProctoringJobData from '@/lib/proctoring/types'
   - Import ProctorEventType from '@prisma/client'

   - Create Redis connection (same pattern as lib/queue.ts)
   - Create Worker on 'proctoring-analysis' queue:
     ```typescript
     const worker = new Worker<ProctoringJobData>(
       'proctoring-analysis',
       async (job: Job<ProctoringJobData>) => {
         const { attemptId, snapshotUrl, timestamp, sensitivity } = job.data

         // 1. Analyze snapshot with GPT-4 Vision
         const analysis = await analyzeSnapshot(snapshotUrl, sensitivity)

         // 2. Determine which flags to create based on sensitivity
         const flags = shouldFlag(analysis, sensitivity)

         // 3. Create ProctorEvent for each flag
         for (const flag of flags) {
           await prisma.proctorEvent.create({
             data: {
               attemptId,
               type: flag as ProctorEventType,
               timestamp: new Date(timestamp),
               metadata: {
                 confidence: analysis.confidence,
                 description: analysis.description,
                 snapshotUrl,
                 faces: analysis.faces,
                 aiGenerated: true
               }
             }
           })
         }

         // 4. Update anti-cheat score on the attempt (optional optimization)
         // Don't do this here — the anti-cheat route recalculates from events

         return { flags, analysis: { faces: analysis.faces, confidence: analysis.confidence } }
       },
       {
         connection: redisConnection,
         concurrency: 3,           // Limit concurrent OpenAI calls
         limiter: {
           max: 10,
           duration: 60000         // Max 10 analyses per minute (cost control)
         }
       }
     )
     ```

   - Add event listeners and ready logging:
     ```typescript
     worker.on('ready', () => {
       console.log('[Proctoring Worker] Worker ready, listening on proctoring-analysis queue')
     })
     worker.on('failed', (job, error) => {
       console.error(`[Proctoring Worker] Job ${job?.id} failed:`, error.message)
     })
     worker.on('completed', (job, result) => {
       console.log(`[Proctoring Worker] Job ${job?.id} completed: ${result.flags.length} flags`)
     })
     ```

   - Add graceful shutdown:
     ```typescript
     process.on('SIGTERM', async () => {
       await worker.close()
       await redisConnection.quit()
       process.exit(0)
     })
     ```

3. Add a script to package.json for running the worker:
   - Add `"worker:proctoring": "npx tsx workers/proctoring-worker.ts"` to scripts
   - Check how the existing grading/export worker is started and follow the same pattern
   - If there's a combined worker runner, add proctoring to it instead

IMPORTANT: Check if there's already a worker runner file (like a combined worker that handles multiple queues). If so, add proctoring to that file instead of creating a standalone worker. If not, create the standalone worker.
  </action>
  <verify>
    Run `npx tsc --noEmit` — no type errors.
    Grep for "proctoring-analysis" in workers/.
    Check package.json for worker script.
    The worker file should import and use both analyzeSnapshot and shouldFlag.
    Run the worker briefly to confirm startup: `npx tsx workers/proctoring-worker.ts &` then check output for "Worker ready" log (kill after 5 seconds). If Redis is unavailable, verify error message is a Redis connection error (not a code/import error).
  </verify>
  <done>
    BullMQ worker processes proctoring-analysis jobs. Calls GPT-4 Vision for analysis. Creates ProctorEvent records for flags based on sensitivity. Rate-limited to 10/minute for cost control. Graceful shutdown on SIGTERM. Worker logs "Worker ready" on successful startup with valid Redis connection.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes
2. workers/proctoring-worker.ts exists with Worker on 'proctoring-analysis'
3. lib/proctoring/snapshotAnalysis.ts exports analyzeSnapshot and shouldFlag
4. Worker creates ProctorEvents with snapshotUrl and confidence in metadata
5. Sensitivity levels filter flags correctly
6. Rate limiting set at 10 analyses per minute
7. Parse errors in analyzeSnapshot return default result with confidence 0 (no throw)
8. Worker startup produces "Worker ready" log (or Redis connection error if Redis unavailable)
</verification>

<success_criteria>
- GPT-4 Vision analyzes snapshots for faces, looking away, suspicious behavior
- Parse errors return safe default SnapshotAnalysisResult with confidence 0, never throw
- shouldFlag filters by sensitivity (low: absence/multi-face, medium: +looking away, high: +suspicious)
- shouldFlag returns empty array when confidence is 0
- BullMQ worker processes jobs with concurrency 3 and rate limit 10/min
- ProctorEvents created with metadata: { confidence, description, snapshotUrl, faces, aiGenerated: true }
- Worker runs via npm script and logs "Worker ready" on startup
- Errors handled with retries (3 attempts, exponential backoff)
</success_criteria>

<output>
After completion, create `.planning/phases/07-intelligent-proctoring/07-04-SUMMARY.md`
</output>
