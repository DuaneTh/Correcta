---
phase: 08-pdf-exam-import
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/exam-import/schemas.ts
  - lib/exam-import/extractor.ts
  - lib/queue.ts
  - scripts/pdf-import-worker.ts
autonomous: true

must_haves:
  truths:
    - "PDF exam content can be extracted into structured question data via GPT-4o"
    - "Extraction schema captures question text, type, points, correction guidelines, and MCQ choices"
    - "PDF import jobs are queued and processed asynchronously via BullMQ"
  artifacts:
    - path: "lib/exam-import/schemas.ts"
      provides: "Zod schemas for exam extraction structured output"
      exports: ["ExamExtractionSchema", "ExtractedQuestionSchema"]
    - path: "lib/exam-import/extractor.ts"
      provides: "GPT-4o PDF analysis function with structured output"
      exports: ["extractExamFromPDF"]
    - path: "lib/queue.ts"
      provides: "pdfImportQueue added alongside existing queues"
      exports: ["pdfImportQueue"]
    - path: "scripts/pdf-import-worker.ts"
      provides: "BullMQ worker that processes PDF import jobs"
      contains: "new Worker('pdf-import'"
  key_links:
    - from: "lib/exam-import/extractor.ts"
      to: "lib/grading/openai-client.ts"
      via: "getOpenAIClient import"
      pattern: "getOpenAIClient"
    - from: "lib/exam-import/extractor.ts"
      to: "lib/exam-import/schemas.ts"
      via: "zodResponseFormat with ExamExtractionSchema"
      pattern: "zodResponseFormat.*ExamExtractionSchema"
    - from: "scripts/pdf-import-worker.ts"
      to: "lib/exam-import/extractor.ts"
      via: "extractExamFromPDF call"
      pattern: "extractExamFromPDF"
    - from: "scripts/pdf-import-worker.ts"
      to: "prisma"
      via: "exam + question + segment creation"
      pattern: "prisma\\.exam\\.create"
---

<objective>
Create the backend extraction pipeline for PDF exam import: Zod schema for structured extraction, GPT-4o extractor function, BullMQ queue registration, and worker script.

Purpose: This is the core AI pipeline that converts a PDF exam document into structured exam data (questions, points, types, correction guidelines) stored in the database. Without this, nothing else in Phase 8 can function.

Output: Four files implementing the complete backend extraction pipeline, following proven patterns from Phase 4 (OpenAI structured outputs) and Phase 5 (BullMQ workers).
</objective>

<execution_context>
@C:\Users\hugol\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\hugol\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-pdf-exam-import/08-RESEARCH.md

# Key pattern references
@lib/grading/openai-client.ts
@lib/grading/grader.ts
@lib/grading/schemas.ts
@lib/queue.ts
@scripts/ai-grading-worker.ts
@lib/storage/minio.ts
@prisma/schema.prisma
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create extraction schemas and GPT-4o extractor function</name>
  <files>lib/exam-import/schemas.ts, lib/exam-import/extractor.ts</files>
  <action>
Create `lib/exam-import/schemas.ts` with two Zod schemas:

1. `ExtractedQuestionSchema` - z.object with fields:
   - `questionNumber`: z.number() - extracted question number from PDF
   - `content`: z.string() - question text, may include LaTeX with $...$
   - `type`: z.enum(['TEXT', 'MCQ']) - detected question type (use TEXT not OPEN_QUESTION to match QuestionType enum in schema.prisma)
   - `maxPoints`: z.number().nullable() - points if found in PDF, null if not stated
   - `correctionGuidelines`: z.string().nullable() - extracted rubric/answer key if present
   - `choices`: z.array(z.object({ text: z.string(), isCorrect: z.boolean().nullable() })).nullable() - MCQ options if type is MCQ, isCorrect is nullable since PDF may not have answer key

2. `ExamExtractionSchema` - z.object with fields:
   - `title`: z.string() - exam title extracted from PDF
   - `questions`: z.array(ExtractedQuestionSchema)
   - `totalPoints`: z.number().nullable() - total if stated in PDF
   - `metadata`: z.object({ confidence: z.enum(['high', 'medium', 'low']), warnings: z.array(z.string()) })

Export types: `ExamExtraction` and `ExtractedQuestion` via z.infer.

All `.describe()` strings should be in French (matching Phase 4 convention) to guide GPT-4o extraction.

Create `lib/exam-import/extractor.ts`:
- Import `getOpenAIClient` from `@/lib/grading/openai-client`
- Import `zodResponseFormat` from `openai/helpers/zod`
- Import `ExamExtractionSchema` from `./schemas`
- Import `getPresignedDownloadUrl` from `@/lib/storage/minio`
- Import `logAIInteraction` from `@/lib/grading/ai-logger`

Export async function `extractExamFromPDF(params: { pdfKey: string, userId?: string }): Promise<ExamExtraction>`:
1. Get OpenAI client via `getOpenAIClient()`
2. Generate presigned download URL from MinIO with 1 hour expiry using `getPresignedDownloadUrl(DEFAULT_BUCKET, params.pdfKey, 3600)`
3. Build system prompt in French instructing GPT-4o to extract exam structure: questions, points, types (TEXT vs MCQ), correction guidelines. Explicitly instruct: "Si une information est ambigue ou absente, utilise null plutot que d'inventer." and "Les formules mathematiques doivent etre en LaTeX avec $...$"
4. Build user prompt asking to analyze the PDF
5. Call `openai.chat.completions.parse()` with:
   - model: 'gpt-4o'
   - messages: system + user with image_url content type pointing to presigned URL
   - response_format: zodResponseFormat(ExamExtractionSchema, 'exam_extraction')
   - temperature: 0.1
   - max_tokens: 4000
6. Log interaction via `logAIInteraction` with operation: 'PDF_IMPORT'
7. Return parsed result, throw if parsing fails

Follow the exact error handling pattern from `lib/grading/grader.ts` (try-catch with logging on both success and failure).
  </action>
  <verify>
Run `npx tsc --noEmit` to verify TypeScript compilation passes. Check that both files exist and export the expected types/functions.
  </verify>
  <done>
`lib/exam-import/schemas.ts` exports ExamExtractionSchema, ExtractedQuestionSchema, ExamExtraction, ExtractedQuestion types. `lib/exam-import/extractor.ts` exports extractExamFromPDF function that calls GPT-4o with native PDF support and structured outputs. TypeScript compiles without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add pdfImportQueue to queue.ts and create worker script</name>
  <files>lib/queue.ts, scripts/pdf-import-worker.ts</files>
  <action>
Modify `lib/queue.ts`:
- Add a new `pdfImportQueue` export alongside existing `aiGradingQueue` and `exportQueue`
- Use same `connection` variable (already shared)
- Queue name: 'pdf-import'
- defaultJobOptions: attempts: 2, backoff: { type: 'fixed', delay: 5000 }, removeOnComplete: { age: 3600, count: 50 }, removeOnFail: { age: 24 * 3600, count: 100 }
- Add null check warning like existing queues
- Update `closeQueue()` to also close pdfImportQueue

Create `scripts/pdf-import-worker.ts` following the exact pattern of `scripts/ai-grading-worker.ts`:
- Import Worker, Job from 'bullmq', Redis from 'ioredis', prisma from '@/lib/prisma'
- Import `extractExamFromPDF` from '@/lib/exam-import/extractor'
- Import `isOpenAIConfiguredSync` from '@/lib/grading/openai-client'
- Connect to Redis with same pattern as ai-grading-worker
- Create Worker on 'pdf-import' queue with concurrency: 1 (PDF analysis is heavy)
- Job handler for 'import-exam' jobs with data: { userId, pdfKey, institutionId, courseId }:
  1. Log start: `[PDF Import Worker] Processing import for PDF ${pdfKey}`
  2. Call `extractExamFromPDF({ pdfKey, userId })`
  3. Create exam in database via prisma.exam.create with:
     - title: extracted.title
     - courseId: job.data.courseId
     - status: 'DRAFT'
     - authorId: userId
     - Create a default section with title '__DEFAULT__', isDefault: true, order: 0
  4. For each extracted question, create Question + QuestionSegment(s):
     - For TEXT questions: create Question with type: 'TEXT', content as JSON segments format (match existing pattern: JSON.stringify([{type:'text', text: q.content}])), order: index, generatedRubric: q.correctionGuidelines ? { criteria: q.correctionGuidelines } : undefined (store extracted guidelines in the existing generatedRubric Json field on Question)
     - Create one QuestionSegment per question with instruction: q.content, maxPoints: q.maxPoints
     - For MCQ questions: create Question with type: 'MCQ', then create QuestionSegments for each choice (instruction: choice.text, isCorrect: choice.isCorrect, order: index)
  5. Return { examId: exam.id, questionCount: extracted.questions.length, confidence: extracted.metadata.confidence, warnings: extracted.metadata.warnings }
- Add completed/failed event handlers with console.log/console.error
- Handle errors: wrap in try-catch, log with `[PDF Import Worker]` prefix, re-throw for BullMQ retry

Important: The content field on Question is a string. Use JSON.stringify for ContentSegments format (array of {type, text} objects) matching the existing pattern used throughout the codebase.
  </action>
  <verify>
Run `npx tsc --noEmit` to verify TypeScript compilation. Verify `lib/queue.ts` exports `pdfImportQueue`. Verify `scripts/pdf-import-worker.ts` exists and imports from correct paths.
  </verify>
  <done>
`lib/queue.ts` exports pdfImportQueue alongside existing queues, closeQueue handles all three. `scripts/pdf-import-worker.ts` processes 'import-exam' jobs: extracts exam from PDF via GPT-4o, creates exam + section + questions + segments in database, returns examId. TypeScript compiles.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. `lib/exam-import/schemas.ts` defines ExamExtractionSchema with proper Zod types
3. `lib/exam-import/extractor.ts` calls OpenAI with image_url content type for PDF
4. `lib/queue.ts` exports pdfImportQueue
5. `scripts/pdf-import-worker.ts` creates exam with questions in database
</verification>

<success_criteria>
- All 4 files exist with correct exports
- TypeScript compiles without errors
- Extraction schema captures all IMPORT-02, IMPORT-03, IMPORT-04 requirements (questions, types, points, guidelines)
- Worker creates proper database records matching existing Prisma schema
- Patterns match existing Phase 4 (grading) and Phase 5 (export) implementations
</success_criteria>

<output>
After completion, create `.planning/phases/08-pdf-exam-import/08-01-SUMMARY.md`
</output>
